# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /datamodule: pred.yaml
  - override /model: pred.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

task_name: "train_pred_TLA_skin_base_new_seg"

num_folds: 5

crop: True

ordinal: True

class_weighted: False

method: 'seg'

net: 'EN_New'

seg_type: "skin"

perturbation: "base"

dataset: "TLA"

tags: ["pred", "skin", "base", "TLA", "EczemaNet_New", 'crop']

seed: 37

trainer:
  accelerator: gpu
  devices: 1
  min_epochs: 10
  max_epochs: 50
#  gradient_clip_val: 0.5

datamodule:
#  batch_size: 32
  seg_type: ${seg_type}
  crop: ${crop}
  perturbation: ${perturbation}
  ordinal: ${ordinal}
  class_weighted: ${class_weighted}
  method: ${method}
  num_folds: ${num_folds}
  k: 0

model:
#  optimizer:
#    lr: 0.03
#    momentum: 0.9
#    weight_decay: 0.005
  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: max
    factor: 0.3
    patience: 5
  model:
    _target_: src.models.components.eczemanet_new.EczemaNet_New
    num_classes: 4
    dropout: 0.5
  monitor: "val/R2"
  n_classes: 4
  ordinal: ${ordinal}
  crop: ${crop}

callbacks:
  model_checkpoint:
    monitor: "val/R2"
    mode: "max"
    save_last: True
    auto_insert_metric_name: False
  early_stopping:
    monitor: "val/R2"
    patience: 10
    mode: "max"

logger:
  wandb:
    tags: ${tags}
    project: "ENET_pred"
    group: ${task_name}
    id: null # pass correct id to resume experiment!
#    notes: train only on man crops
